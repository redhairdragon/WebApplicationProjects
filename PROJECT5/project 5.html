<!DOCTYPE html>
<!-- saved from url=(0056)http://oak.cs.ucla.edu/classes/cs144/project5/index.html -->
<html lang="en"><!-- ***IMPORTANT***: This page is autogenerated from a markdown file
         DO NOT EDIT THIS FILE DIRECTLY.
	 Your edits will disappear when this page is regenerated  --><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        code{white-space: pre-wrap;}
        span.smallcaps{font-variant: small-caps;}
        span.underline{text-decoration: underline;}
        div.line-block{white-space: pre-line;}
        div.column{display: inline-block; vertical-align: top; width: 50%;}
        pre{background-color:#e7e7e7; border: solid 1px #000000; padding: 5px;}
        code{background-color:#e7e7e7; padding: 1px;}
        div.sourceCode { overflow-x: auto; }
        table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
          margin: 0; padding: 0; vertical-align: baseline; border: none; }
        table.sourceCode { width: 100%; line-height: 100%; }
        td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
        td.sourceCode { padding-left: 5px; }
        code > span.kw { color: #0000ff; } /* Keyword */
        code > span.ch { color: #008080; } /* Char */
        code > span.st { color: #008080; } /* String */
        code > span.co { color: #008000; } /* Comment */
        code > span.ot { color: #ff4000; } /* Other */
        code > span.al { color: #ff0000; } /* Alert */
        code > span.er { color: #ff0000; font-weight: bold; } /* Error */
        code > span.wa { color: #008000; font-weight: bold; } /* Warning */
        code > span.cn { } /* Constant */
        code > span.sc { color: #008080; } /* SpecialChar */
        code > span.vs { color: #008080; } /* VerbatimString */
        code > span.ss { color: #008080; } /* SpecialString */
        code > span.im { } /* Import */
        code > span.va { } /* Variable */
        code > span.cf { color: #0000ff; } /* ControlFlow */
        code > span.op { } /* Operator */
        code > span.bu { } /* BuiltIn */
        code > span.ex { } /* Extension */
        code > span.pp { color: #ff4000; } /* Preprocessor */
        code > span.do { color: #008000; } /* Documentation */
        code > span.an { color: #008000; } /* Annotation */
        code > span.cv { color: #008000; } /* CommentVar */
        code > span.at { } /* Attribute */
        code > span.in { color: #008000; } /* Information */
    </style>
    <link rel="stylesheet" href="./project 5_files/pandoc.css">
    <title></title>
</head>
<body>
<h1 id="project-5-web-benchmark-and-spark">Project 5: Web Benchmark and Spark</h1>
<h2 id="change-history">Change History</h2>
<ol type="1">
<li>3/5/2018 11:25PM: Fixed the reversed read/write ratio for mixed_tomcat.py</li>
<li>3/7/2018 1:25PM: 400ms-response-time requirement is dropped for the requests to <code>/login?...</code></li>
<li>3/9/2018 1:50PM: Added clarification in case the maximum # users is below 100</li>
</ol>
<h2 id="overview">Overview</h2>
<p>An important part of any Web application development is to provision our Web site for the expected load from the users. To do this, we need to be able to measure how many concurrent user requests a single Web server can handle, so that we can estimate how many server instances are needed to handle the expected load. It is also important to be able to run any complex data processing tasks on a clusters of machines available to us, so that we can provide a near-time feedback from our user data to improve our application.</p>
<p>In Part A, we would focus on measuring the number of concurrent requests that a system can handle. More precisely, we will look into measuring how many users would get the response within a reasonable time period.</p>
<p>In Part B, we would learn how we can use the popular Apache Spark engine to run a complex data processing tasks on a clusters of machine in parallel.</p>
<h2 id="development-environment">Development Environment</h2>
<p>The main development of Project 5 will be done using a new docker container named "spark" created from "junghoo/cs144-spark". Use the following command to create and start this new container:</p>
<pre class="term"><code>$ docker run -it -p4040:4040 -p8089:8089 -v {host_shared_dir}:/home/cs144/shared --name spark junghoo/cs144-spark</code></pre>
<p>Make sure to replace <code>{host_shared_dir}</code> with the name of the shared directory on your host. The above command creates a docker container named <code>spark</code> with appropriate port forwarding and directory sharing.</p>
<p>Our new container has Locust (v0.8.1), JDK (v1.8.0), Scala (v2.12.2), and Spark (v2.2.0) pre-installed. You can check their versions by the following commands.</p>
<pre class="term"><code>$ locust --version
$ java -version
$ scala -version
$ spark-shell --version</code></pre>
<p>As before, the default username inside the container is "cs144" with password "password".</p>
<p>When you perform the tasks for Parts A and B of Project 5, you also need to run the servers you developed in Projects 2 and 4. Make sure that your servers in the "tomcat" and "mean" containers are still available and run well by executing the following sequence of commands inside the <em>host</em>:</p>
<pre class="term"><code>$ docker start tomcat
$ docker start mean
$ docker exec -d mean sudo mongod --fork --logpath /var/log/mongodb/mongodb.log
$ docker exec -d mean npm start --prefix {your_blog_server_dir}</code></pre>
<p>Replace <code>{your_blog_server_dir}</code> with the path to your Project 4 directory inside the mean container, e.g., <code>/home/cs144/shared/project4/blog-server</code>. Make sure that your Tomcat server is available at <a href="http://localhost:8888/" class="uri">http://localhost:8888</a> and your node server is available at <a href="http://localhost:3000/" class="uri">http://localhost:3000</a>.</p>
<p>By default, each container runs in its own isolated environment and can reach other containers only if it knows their IP addresses. In this project, we want to make the containers reachable by their <em>container names</em>, not just by IPs, so that we can send HTTP requests between containers more easily. This can be done by creating a custom <em>bridge network</em> and connecting our three containers to the bridge network. Run the following command inside the <em>host</em> to create a custom bridge network, named <code>cs144-net</code>:</p>
<pre class="term"><code>$ docker network create cs144-net</code></pre>
<p>Once a bridge network is created, any running container can be connected to it through the <code>docker network connect</code> command:</p>
<pre class="term"><code>$ docker network connect cs144-net spark
$ docker network connect cs144-net tomcat
$ docker network connect cs144-net mean</code></pre>
<p>The above sequence of commands will connect our three containers, "spark", "tomcat", and "mean", to the bridge network "cs144-net". (Make sure that the three containers are already running before you execute the above commands.)</p>
<p>Now the three containers can reach each other through their names. To verify, execute a shell in the "spark" container and run <code>curl</code> commands to issue HTTP requests to other containers like the following:</p>
<pre class="term"><code>$ docker exec -it spark /bin/bash
cs144@e6a142ac3bf9:~$ curl http://mean:3000/
cs144@e6a142ac3bf9:~$ curl http://tomcat:8080/</code></pre>
<p>The first <code>docker exec</code> command runs a shell in the spark container, so that the next commands are executed from the container, not from the host. The next two <code>curl</code> commands, therefore, issue HTTP requests from the spark container to the mean and tomcat containers. As long as the mean and tomcat containers have started and are running, you should see correct responses from the servers of the other two containers. Make sure that this is the case by checking the responses.</p>
<p>Once you finish checking the containers and their network connectivity, you can stop them using the following command:</p>
<pre class="term"><code>$ docker stop spark
$ docker stop mean
$ docker stop tomcat</code></pre>
<p>From now on, all three containers can talk to each other through their names!</p>
<h2 id="part-a-testing-performance-of-servers">Part A: Testing Performance of Server(s)</h2>
<p>In this part, you are require to (1) learn how to use Locust to test the performance of your server, (2) write some customized files that represent user behavior, (3) record the results of test cases, and (4) find the maximum number of users that the server could handle under some performance requirements.</p>
<h3 id="learn-to-use-locust">Learn to Use Locust</h3>
<p>You <strong>must</strong> go over the following Locust tutorial, step-by-step, before moving on:</p>
<ul>
<li><a href="http://oak.cs.ucla.edu/classes/cs144/locust/index.html">Quick Locust Tutorial</a></li>
</ul>
<h3 id="mock-data-preparation">Mock Data Preparation</h3>
<p>Before we start load testing our servers using Locust, we need to load some mock data into our databases. We prepared two scripts, <a href="http://oak.cs.ucla.edu/classes/cs144/project5/mock_data_tomcat.sh">mock_data_tomcat.sh</a> and <a href="http://oak.cs.ucla.edu/classes/cs144/project5/mock_data_node.sh">mock_data_node.sh</a> for this purpose. Download the tomcat script inside the "tomcat" container and execute it like the following:</p>
<pre class="term"><code>$ wget http://oak.cs.ucla.edu/classes/cs144/project5/mock_data_tomcat.sh
$ bash ./mock_data_tomcat.sh</code></pre>
<p>Make sure that MySQL and Tomcat servers are up and running inside the container before you run the above script.</p>
<p>Do the same for the node script inside the "mean" container:</p>
<pre class="term"><code>$ wget http://oak.cs.ucla.edu/classes/cs144/project5/mock_data_node.sh
$ bash ./mock_data_node.sh</code></pre>
<p>Again, make sure that MongoDB server is running inside the container before you run the above script.</p>
<p>Now our databases have been populated with 500 fake blog posts by the user <code>cs144</code>.</p>
<h3 id="write-locust-files">Write Locust Files</h3>
<p>Once we have all the test data inserted, it's time for you to write locust files to load test the servers. In particular, you are required to write 6 files to perform the following tests:</p>
<h4 id="locust-file-for-tomcat-server">Locust File for Tomcat Server</h4>
<ol type="1">
<li><p><em>read_tomcat.py</em></p>
<ul>
<li><p>In this file, we are simulating the scenario where all requests from users are read intensive. The user whose name is <code>cs144</code> would randomly open one of his posts via <code>/editor/post?action=open&amp;username=cs144&amp;postid={num}</code>, where <code>{num}</code> is a random postid.</p></li>
<li><p><strong>Note</strong>: In this test file, use <code>/editor/post?action=open</code> as the <code>name</code> for the requests. Also, make sure that postid that user opens should be between 1 and 500. Since our user "cs144" only has 500 posts, he will get nothing otherwise!</p></li>
</ul></li>
<li><p><em>write_tomcat.py</em></p>
<ul>
<li><p>In this test file, we are simulating the scenario where the requests from users are write intensive. The user <code>cs144</code> would modify one of his posts randomly by changing the title to "Loading Test" and the body to "***Hello World!***" via <code>/editor/post?action=save&amp;username=cs144&amp;postid={num}&amp;title=Loading%20Test&amp;body=***Hello%20World!***</code>. Replace <code>{num}</code> with a random number between 1 and 500.</p></li>
<li><p><strong>Note</strong>: Use <code>/editor/post?action=save</code> as the <code>name</code> for this request. You may use <em>only</em> POST method here since GET method is not supported.</p></li>
</ul></li>
<li><p><em>mixed_tomcat.py</em></p>
<ul>
<li><p>In this test file, we are simulating a more realistic scenario where some users are reading posts while others are writing. In this test, 10% of users are write intensive while the remaining 90% are read intensive.</p></li>
<li><p><strong>Note</strong>: Just "cut and paste" the tasks/functions you wrote in previous files and combine them with different weights. 10% of user tasks must come from the "write task" defined in <em>write_tomcat.py</em> and the remaining 90% of user tasks must come from the "read task" defined in <em>read_tomcat.py</em>.</p></li>
</ul></li>
</ol>
<h4 id="locust-file-for-node-server">Locust File for Node Server</h4>
<ol start="4" type="1">
<li><p><em>read_node.py</em></p>
<ul>
<li><p>In this test file, we are simulating a similar behavior of <code>read_tomcat.py</code> except that now we are testing the performance of our Node.JS server. The user <code>cs144</code> would randomly open one of her posts via <code>/blog/:username/:postid</code> API. Again, remember to limit the postid between 1 and 500 when you pick one randomly.</p></li>
<li><p><strong>Note</strong>: Use <code>/blog/cs144</code> as the <code>name</code> for this request.</p></li>
</ul></li>
<li><p><em>write_node.py</em></p>
<ul>
<li><p>In this file, we are going test the server performance under write intensive requests as we did in <code>write_tomcat.py</code>. The user <code>cs144</code> randomly update one of her posts by changing its title to "Loading Test" and body to "***Hello World!***" through <code>/api/:username/:postid</code>. Remember that you need to obtain the JWT authentication token through the login page before calling this API. Limit the postid to between 1 and 500.</p></li>
<li><p><strong>Note</strong>: Use <code>/api/cs144 (update)</code> as <code>name</code> for the request.</p></li>
</ul></li>
<li><p><em>mixed_node.py</em></p>
<ul>
<li><p>In this file, we combine the read intensive tasks and write intensive ones in a single file as you would expect. The percentage would remain the same as 10% write and 90% read.</p></li>
<li><p><strong>Note</strong>: Just reuse the tasks/functions you wrote in previous files and combine them with different weights.</p></li>
</ul></li>
</ol>
<h3 id="run-locust-files-and-save-results">Run Locust Files and Save Results</h3>
<h4 id="part-1">Part 1</h4>
<p>Once you fish writing the six locust files, run Locust without the web UI using the following four files: <em>read_tomcat.py</em>, <em>write_tomcat.py</em>, <em>read_node.py</em>, and <em>write_node.py</em>. Once they finish running, fill in the <a href="http://oak.cs.ucla.edu/classes/cs144/project5/performance.json">performance.json</a> file with your results, where the field values are 0. More precisely, you should fill in the fields, <em>"Number of Users"</em> and <em>"Total Number Of Req"</em>, according to the configuration parameters you set in the command, and, <em>"Total RPS"</em> and <em>"response time for 98% of the requests"</em>, with the numbers that you get from the final summary of your tests.</p>
<p><strong>Note</strong>: The filled values should be <strong>numbers</strong>, not a string. For example, if your total RPS is 160 req/s, just put the number 160, like "Total RPS": 160, <strong>NOT</strong> like "Total RPS": "160 req/s". Our grading script won't correctly recognize any value other than numbers. Do <strong>NOT</strong> change the any other part of the <a href="http://oak.cs.ucla.edu/classes/cs144/project5/performance.json">performance.json</a> file.</p>
<h4 id="part-2">Part 2</h4>
<p>After filling in the <a href="http://oak.cs.ucla.edu/classes/cs144/project5/performance.json">performance.json</a>, you should be now familiar with the testing procedure. Let's try to find the maximum users that the server could handle under 10%-write and 90%-read load using <em>mixed_tomcat.py</em> and <em>mixed_node.py</em> files. In particular, we require that <strong><em>the servers must return at least 98% responses within less than 400ms for all requests except login requests</em></strong>. That is, each URL name group except <code>/login?...</code> should return 98% of responses in less than 400ms.</p>
<p>Your task is to find the maximum number of users (in the unit of hundreds like 300 if the number if higher than 100, in the unit of tens like 60 if the number if higher than 10, and in the unit of 1 if it is below 10) that the servers can handle under this requirement. Use <em>summary_tomcat.txt</em> and <em>summary_node.txt</em> to save the results from the tests when the server met this requirement with the largest number of users. Please <strong><em>save only the summary in your result files by using <code>--only-summary</code>.</em></strong></p>
<p>After Part A is done, you should have 6 test files (<em>read_tomcat.py, write_tomcat.py, mixed_tomcat.py, read_node.py, write_node.py, mixed_node.py</em>), 1 JSON file (<em>performance.json</em>) and 2 summary files (<em>summary_tomcat.txt, summary_node.txt</em>).</p>
<p><strong>Note</strong>: There is no right or wrong results for performance stats in Part A as long as your test files are correctly implemented. You just need to submit the results that you get from your performance benchmark.</p>
<p><strong>Note</strong>: Now that you are done with Part A, think about the performance of two servers in each scenario. Which one exhibited a better performance? Tomcat or Node.JS? Note that our comparison may be a bit unfair to the Node.JS server because (1) Tomcat did not "render" markdown to HTML when a blog post is "opened" and (2) Node.JS had to perform the extra authentication verification step for the "write" tasks. You may want to keep this difference in mind when you interpret your results. You do not provide an answer to this question in your submission. This question is to encourage you to think more deeply about what you observed from load testing.</p>
<h2 id="part-b-apache-spark">Part B: Apache Spark</h2>
<p>In this part, you will learn how to use the popular <a href="http://spark.apache.org/">Apache Spark engine</a> to perform a (potentially heavy) computational task on a large number of machines using the Map-Reduce framework. In particular, you will identify the most "popular" users on the Twitter network, measured by the number of their followers. Our new "spark" container has the Spark engine preinstalled. We also provide a snapshot of the follower-following graph of Twitter. Your job is to write a (simple) code on Spark that returns the IDs of the users with the high follower counts.</p>
<h3 id="twitter-graph-file-and-our-task">Twitter Graph File and Our Task</h3>
<p>Download the <a href="http://oak.cs.ucla.edu/classes/cs144/project5/twitter.edges" class="uri">twitter.edges</a> file that contains a snapshot of the follower-following graph structure of Twitter:</p>
<ul>
<li><a href="http://oak.cs.ucla.edu/classes/cs144/project5/twitter.edges" class="uri">twitter.edges</a>: Twitter follower-following graph data</li>
</ul>
<p>Each line of the file represents the "following" edges from a particular Twitter user in the format below:</p>
<pre><code>user1: user2,user3,...,userk</code></pre>
<p>The above line indicates that user1 is "following" user2 through userk. Note that each user in the file is represented as a unique random integer. For example, the first line of the file:</p>
<pre><code>138134695: 62902439,15224867</code></pre>
<p>indicates that the user "138134695" is following two other users, 62902439 and 15224867.</p>
<p>Given this file, it is relatively straightforward to find the user who follows the largest number of users. We simply need to identify the line with the largest of user IDs behind colon. Unfortunately, our task is more complex. <strong>We need to identify the users who are followed by a large number of other users (more precisely, 1,000 other users).</strong> While our dataset is reasonably small -- it is only 21 MB in size -- you can imagine that this dataset can potentially be huge, so we want to implement this task using the Apache Spark Engine, so that we can perform this task in parallel on as many available machines as possible.</p>
<h3 id="apache-spark">Apache Spark</h3>
<p>Writing a program that runs on a large number of machines in parallel can be a daunting task. To make this task easier, a number of distributed software infrastructures have been developed. In particular, as we learned in class, Map-Reduce framework asks the programmer to provide just the core computational logic of the given task as a set of Map and Reduce functions. Given this core functions, Map-Reduce framework takes care of the rest, including data distribution, parallel execution, process monitoring, and result shuffling and collection. Apache Spark is a popular open source software that provides a Map-Reduce style programming environment on a cluster with a large number of distributed machines.</p>
<p>Once you are inside our <code>spark</code> container, you can run the Spark interactive shell by executing the <code>spark-shell</code> command:</p>
<pre class="term"><code>$ spark-shell
Spark context Web UI available at http://172.17.0.2:4040
Spark context available as 'sc' (master = local[*], app id = local-1518992867666).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/_,_/_/ /_/_\   version 2.2.0
      /_/

Using Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_151)
Type in expressions to have them evaluated.
Type :help for more information.

scala&gt;</code></pre>
<p>Inside the Spark shell, you can execute any scala command using Spark API. You can exit from the Spark interactive shell by pressing "Control+D" key.</p>
<p>Now that we know how to start the Apache Spark Shell, it is time to learn how to use it by going over one of widely available Apache Spark tutorials on the Internet. For example, the official <a href="http://spark.apache.org/docs/latest/quick-start.html">Quick Start Tutorial</a> provides a ten-minute introduction to essential basics. The file <a href="http://oak.cs.ucla.edu/classes/cs144/project5/wordCount.scala" class="uri">wordCount.scala</a> also contains the example code that we went over in the class:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> lines = sc.<span class="fu">textFile</span>(<span class="st">"input.txt"</span>)
<span class="kw">val</span> words = lines.<span class="fu">flatMap</span>(line =&gt; line.<span class="fu">split</span>(<span class="st">" "</span>))
<span class="kw">val</span> word1s = words.<span class="fu">map</span>(word =&gt; (word, <span class="dv">1</span>))
<span class="kw">val</span> wordCounts = word1s.<span class="fu">reduceByKey</span>((a,b) =&gt; a+b)
wordCounts.<span class="fu">saveAsTextFile</span>(<span class="st">"output"</span>)
System.<span class="fu">exit</span>(<span class="dv">0</span>)</code></pre></div>
<p>Again, the above code computes the frequency of each word in the text file <a href="http://oak.cs.ucla.edu/classes/cs144/project5/input.txt" class="uri">input.txt</a> and generates (word, frequency) pairs as the output. When <code>saveAsTextFile("output")</code> is called, the program creates a new subdirectory named <code>output</code>, where <code>part-xxxxx</code> file(s) are generated that contain the (word, frequency) output pairs in <code>wordCounts</code>. Note the last line in the <code>wordCount.scala</code> script: <code>System.exit(0)</code>. Calling the system exit function ensures that once the script finishes, the interactive shell is aborted as well. You can execute this file using the Spark shell using the following command:</p>
<pre class="term"><code>$ spark-shell -i wordCount.scala
...
$ ls -l output/
total 8
-rwxr-xr-x 1 root root    0 Mar 16 03:18 _SUCCESS
-rwxr-xr-x 1 root root 2131 Mar 16 03:18 part-00000
-rwxr-xr-x 1 root root 2056 Mar 16 03:18 part-00001
$ head output/part-00000
(country,2)
(House,2)
(its,1)
(previously,1)
(countries,,1)
(have,2)
(policy,1)
(travel,,1)
(order,7)
(Trumpâ€™s,2)</code></pre>
<p>Note that the provided <code>wordCount.scala</code> code is written to be executed in an interactive shell only. If we want to properly run our program on multiple machines in a Spark cluster, we need to "wrap" this code within an object with a "main" function. We also need to create proper "Spark Configuration" and "Spark Context" in which our program will run. For example, <a href="http://oak.cs.ucla.edu/classes/cs144/project5/wordCountFull.scala" class="uri">wordCountFull.scala</a> shows an extended version of our wordCount program that can be properly compiled into an executable "package", say <code>word-count-project.jar</code> using a tool chain such as <code>sbt</code>. Once packaged, it can be submitted to a Spark cluster for parallel execution using a command like the following:</p>
<pre class="term"><code>$ spark-submit --class edu.ucla.cs144.WordCount --master spark://23.195.26.187:7077 --deploy-mode cluster word-count-project.jar</code></pre>
<p>Since the goal of this project is to introduce you to the main programming paradigm/API of Spark, not the nitty gritty details of the Spark packaging tool chain and job submission interface, in this project we will assume that your code will be executed <strong>through a Spark interactive shell</strong> using the command <code>spark-shell -i</code>.</p>
<p>While Spark supports multiple languages, Scala is the most popular (and syntactically clean) language to program on Spark, so <strong>we use Scala for this project.</strong> Basic Scala needed to complete this project is easy to learn and your code will be much cleaner than when you use other languages. There exist many quick online tutorials on Scala, such as <a href="http://www.scala-lang.org/docu/files/ScalaTutorial.pdf">this one</a>. Fortunately, this project can be completed without using any advanced Scala constructs like class inheritance. In particular, we note that <a href="https://www.tutorialspoint.com/scala/anonymous_functions.htm"><em>anonymous functions</em></a> in Scala are similar to "arrow functions" in JavaScript, which can be used to pass "Map-Reduce" functions.</p>
<h3 id="writing-your-code">Writing Your Code</h3>
<p>Now that you got the basics, it is time to write code. Your code must read the text file <a href="http://oak.cs.ucla.edu/classes/cs144/project5/twitter.edges" class="uri">twitter.edges</a> located in the current directory, parse it to obtain the Twitter follower-following graph, perform necessary computation, and return the list of all (userid, follower_count) pairs for the users with more than 1000 followers. The output from your code should contain many lines of (userid, follower_count) pairs like:</p>
<pre><code>(40981798,8569)
(43003845,7621)
...</code></pre>
<p>The first two lines of the above output, for example, indicate that the users 40981798 and 43003845 have 8569 and 7621 followers, respectively. The output (userid, follower_count) pairs should be saved as a (set of) text file(s) in the "output" directory using the Spark <code>saveAsTextFile()</code> function. <strong>The output (userid, follower_count) pairs may appear in any order and need not be sorted.</strong></p>
<p>In writing your code, you may find the list of <a href="http://spark.apache.org/docs/latest/programming-guide.html#transformations">Spark transformation functions</a> helpful. Also, if you need a "hint" on parsing the provided <a href="http://oak.cs.ucla.edu/classes/cs144/project5/twitter.edges" class="uri">twitter.edges</a> file, you may find <a href="http://stackoverflow.com/questions/29820501/splitting-strings-in-apache-spark-using-scala">this question and answer at StackOverflow</a> helpful (local mirror is <a href="http://oak.cs.ucla.edu/classes/cs144/project5/string-spark.html">available here</a>).</p>
<p>Before we finish, we reiterate the essential requirements of your code.</p>
<h3 id="code-requirements">Code Requirements</h3>
<ol type="1">
<li>Your code should read the twitter graph from the file "twitter.edges" located in the current directory.</li>
<li>The output from your code should be the list of (userid, follower_counts) pairs for all the users whose follower count is larger than 1000. The output does not have to be sorted.</li>
<li>Your code should save the output in the "output" subdirectory within the current working directory using <code>saveAsTextFile()</code> of Spark RDD.</li>
<li>Your code should compute the final results using Map-Reduce-style programming by applying a series of Spark transformation functions to the input dataset.</li>
</ol>
<h3 id="testing-your-code">Testing Your Code</h3>
<p>Before submitting your code, thoroughly test your code so that it computes the correct results. To help you ensure that your code produces the correct output, here are a few sample results from our dataset:</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;">User ID</td>
<td style="text-align: left;">Follower Count</td>
</tr>
<tr class="even">
<td style="text-align: left;">40981798</td>
<td style="text-align: left;">8569</td>
</tr>
<tr class="odd">
<td style="text-align: left;">3359851</td>
<td style="text-align: left;">3905</td>
</tr>
<tr class="even">
<td style="text-align: left;">88323281</td>
<td style="text-align: left;">2315</td>
</tr>
<tr class="odd">
<td style="text-align: left;">18742444</td>
<td style="text-align: left;">1585</td>
</tr>
<tr class="even">
<td style="text-align: left;">9451052</td>
<td style="text-align: left;">1184</td>
</tr>
<tr class="odd">
<td style="text-align: left;">302847930</td>
<td style="text-align: left;">1182</td>
</tr>
<tr class="even">
<td style="text-align: left;">12925072</td>
<td style="text-align: left;">1002</td>
</tr>
</tbody>
</table>
<p>In total, there are 177 users with more than 1000 followers.</p>
<h2 id="what-to-submit">What to Submit</h2>
<p>For this project, you need to submit <strong>a single zip file</strong> named <code>project5.zip</code> that has the following packaging structure.</p>
<pre><code>project5.zip
 +- read_tomcat.py, write_tomcat.py, mixed_tomcat.py
 +- read_node.py, write_node.py, mixed_node.py
 +- performance.json
 +- summary_tomcat.txt, summary_node.txt
 +- topUsers.scala
 +- TEAM.txt
 +- README.txt (optional)</code></pre>
<p>Each file or directory is as following:</p>
<ol type="1">
<li>read_tomcat.py, write_tomcat.py, mixed_tomcat.py, read_node.py, write_node.py, mixed_node.py: These are the six locust files to load test your tomcat and node servers</li>
<li>performance.json: This is the JSON file that contains the load test results.</li>
<li>summary_tomcat.txt, summary_node.txt: These are the summary table from the two mixed benchmark tests of Part A-1</li>
<li><code>topUsers.scala</code>: this is the main Scala code that you wrote to compute the top Twitter users. This code should be executable simply by typing "<code>spark-shell -i topUsers.scala</code>". Please <strong>DO NOT</strong> submit any input or output files for your code. Just submit your main Scala script.</li>
<li>TEAM.txt: This file must include the 9-digit university ID (UID) of every team member, one UID per line. No spaces or dashes. Just 9-digit UID per line. If you are working on your own, include just your UID.</li>
<li><strong><code>README.txt</code></strong> includes any comments you find worth noting, regarding your code structure, etc.</li>
</ol>
<p>All files should be <strong>contained directly</strong> under the <strong>project5.zip</strong> (without any enclosing folders).</p>
<h2 id="testing-your-zip-file">Testing Your Zip File</h2>
<p>To ensure the correct packaging of your submission, we have made a grading script <a href="http://oak.cs.ucla.edu/classes/cs144/project5/p5_test" class="uri">p5_test</a> for Project 5, which can be executed like:</p>
<pre class="term"><code>$ ./p5_test project5.zip</code></pre>
<p>(Add the appropriate path to the project5.zip if needed. You may need to use "chmod +x p5_test" if there is a permission error.)</p>
<p>You <strong>MUST</strong> test your submission using the script to minimize the chance of an unexpected error during grading. When everything runs properly, you will see an output similar to the following from the script:</p>
<pre class="term"><code>Executing your Spark code.....
Spark context Web UI available at http://10.0.2.15:4040
Spark context available as 'sc' (master = local[*], app id = local-1488946788877).
Spark session available as 'spark'.
Loading topUsers.scala...

...

(20,1010)
(99,1010)
(10,1010)

SUCCESS! We finished testing your zip file integrity.</code></pre>
<p>Once your work is properly packaged as a zip file, submit your zip file via our submission page at <a href="http://ccle.ucla.edu/course/view/18W-COMSCI144-1">CCLE</a>.</p>
<p>You may submit as many times as you like, however only the latest submission will be saved, and those are what we will use for grading your work and determining late penalties.</p>
<h2 id="grading-criteria">Grading Criteria</h2>
<p>Overall grading breakdown is as below</p>
<ul>
<li>Part A: 6 test files runs without being terminated by any error (20%)</li>
<li>Part A: performance.json contains your real test results (10%)</li>
<li>Part A: the data and settings in 2 summary files meet the requirements (20%)</li>
<li>Part B: Submitted code runs without any error and produces output (15%)</li>
<li>Part B: Submitted code computes correct answer on the provided dataset (15%)</li>
<li>Part B: Submitted code computes correct answer on different datasets with the same format (20%)</li>
</ul> 


</body></html>